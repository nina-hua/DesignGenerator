{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "\n",
    "This is a first attempt at creating a simple GAN model following the documentation of *Deep Learning with Pytorch* by Vishnu Subramanian and this Medium article: https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f\n",
    "\n",
    "Certain model parameters and structure will be based off of the pix2pix model implemented in *Image-to-Image Translation with Conditional Adversarial Networks* by Isola, et al.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time \n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load re-sized images and their corresponding sketches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/couch_gan\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign paths \n",
    "real_img_path = Path('./data/og_resized')\n",
    "sketch_img_path = Path('./data/sketch_resized')\n",
    "fake_img_path = Path('./data/fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files into list  \n",
    "real_img_files = [f for f in real_img_path.iterdir()]\n",
    "sketch_img_files = [f for f in sketch_img_path.iterdir()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get image names of training and test files (assigned by Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_names = [f'00000{i}.jpg' for i in range(203, 303)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign paths to appropriate lists \n",
    "train_real_paths = []\n",
    "train_sketch_paths = []\n",
    "test_real_paths = []\n",
    "test_sketch_paths = []\n",
    "\n",
    "for f in real_img_files:\n",
    "    if f.parts[-1] in test_file_names:\n",
    "        test_real_paths.append(f)\n",
    "    else:\n",
    "        train_real_paths.append(f)\n",
    "\n",
    "for f in sketch_img_files:\n",
    "    if f.parts[-1] in test_file_names:\n",
    "        test_sketch_paths.append(f)\n",
    "    else:\n",
    "        train_sketch_paths.append(f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that the order of real and sketch paths are the same \n",
    "counter = 0\n",
    "for r, f in zip(train_real_paths, train_sketch_paths):\n",
    "    if r.parts[-1] == r.parts[-1]:\n",
    "        counter += 1\n",
    "assert(counter == 900)\n",
    "\n",
    "counter = 0\n",
    "for r, f in zip(test_real_paths, test_sketch_paths):\n",
    "    if r.parts[-1] == r.parts[-1]:\n",
    "        counter += 1\n",
    "assert(counter == 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Couch Dataset  and Dataloaders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: No data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getX(path):\n",
    "    x = cv2.imread(str(path)).astype(np.float32)  # reading image \n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)/255  # convert from BGR to RGB\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(im):\n",
    "    \"\"\"Normalizes images \"\"\"\n",
    "    imagenet_stats = np.array([[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]])\n",
    "    return (im - imagenet_stats[0])/imagenet_stats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDataset(Dataset):\n",
    "    def __init__(self, real_files, sketch_files):\n",
    "        self.real_files = real_files\n",
    "        self.sketch_files = sketch_files\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.real_files)  # should be same number as sketch files \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        real_path = self.real_files[idx]\n",
    "        sketch_path = self.sketch_files[idx]\n",
    "        img_name = sketch_path.parts[-1]\n",
    "        \n",
    "        x_real = getX(real_path)\n",
    "        x_sketch = getX(sketch_path)\n",
    "        \n",
    "        # normalize \n",
    "        x_real = normalize(x_real)\n",
    "        x_sketch = normalize(x_sketch)\n",
    "        \n",
    "        # roll axis (channels, height, width)\n",
    "        x_real = np.rollaxis(x_real, 2)\n",
    "        x_sketch = np.rollaxis(x_sketch, 2)\n",
    "        \n",
    "        return x_sketch, x_real\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make datasets \n",
    "train_couch_ds = CouchDataset(real_files=train_real_paths, sketch_files=train_sketch_paths)\n",
    "test_couch_ds = CouchDataset(real_files=test_real_paths, sketch_files=test_sketch_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect content \n",
    "x_s, x_r = train_couch_ds[20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]]]),\n",
       " array([[[ 0.87450981,  0.89019608,  0.9137255 , ...,  0.32549024,\n",
       "           0.3176471 ,  0.3176471 ],\n",
       "         [ 0.88235295,  0.89803922,  0.9137255 , ..., -0.09019607,\n",
       "          -0.03529412,  0.0196079 ],\n",
       "         [ 0.84313726,  0.85882354,  0.86666667, ..., -0.36470586,\n",
       "          -0.34117645, -0.31764704],\n",
       "         ...,\n",
       "         [-0.45098037, -0.4588235 , -0.4588235 , ..., -0.02745098,\n",
       "          -0.05098039, -0.06666666],\n",
       "         [-0.46666664, -0.48235291, -0.47450978, ..., -0.05098039,\n",
       "          -0.05882353, -0.05882353],\n",
       "         [-0.4588235 , -0.47450978, -0.47450978, ..., -0.00392157,\n",
       "          -0.01176471, -0.00392157]],\n",
       " \n",
       "        [[ 0.71764708,  0.73333335,  0.75686276, ...,  0.18431377,\n",
       "           0.17647064,  0.17647064],\n",
       "         [ 0.74117649,  0.75686276,  0.77254903, ..., -0.23137254,\n",
       "          -0.17647058, -0.12156862],\n",
       "         [ 0.70980394,  0.72549021,  0.73333335, ..., -0.52156863,\n",
       "          -0.49803919, -0.47450978],\n",
       "         ...,\n",
       "         [-0.4588235 , -0.46666664, -0.46666664, ..., -0.02745098,\n",
       "          -0.05098039, -0.06666666],\n",
       "         [-0.47450978, -0.49019605, -0.48235291, ..., -0.05098039,\n",
       "          -0.05882353, -0.05882353],\n",
       "         [-0.46666664, -0.48235291, -0.48235291, ..., -0.00392157,\n",
       "          -0.01176471, -0.00392157]],\n",
       " \n",
       "        [[ 0.64705884,  0.66274512,  0.68627453, ...,  0.07450986,\n",
       "           0.06666672,  0.06666672],\n",
       "         [ 0.66274512,  0.67843139,  0.69411767, ..., -0.34117645,\n",
       "          -0.28627449, -0.23137254],\n",
       "         [ 0.64705884,  0.66274512,  0.67058825, ..., -0.60784313,\n",
       "          -0.58431372, -0.56078431],\n",
       "         ...,\n",
       "         [-0.49019605, -0.49803919, -0.49803919, ..., -0.02745098,\n",
       "          -0.05098039, -0.06666666],\n",
       "         [-0.50588235, -0.52156863, -0.51372549, ..., -0.05098039,\n",
       "          -0.05882353, -0.05882353],\n",
       "         [-0.49803919, -0.51372549, -0.51372549, ..., -0.00392157,\n",
       "          -0.01176471, -0.00392157]]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_s, x_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 240, 400), (3, 240, 400))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 channels for RGB\n",
    "# 240 rows (height)\n",
    "# 400 columns (width)\n",
    "x_s.shape, x_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on Bicycle GAN \n",
    "## https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/bicyclegan/models.py\n",
    "\n",
    "## U-Net Generator \n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 3, stride=2, padding=1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_size, 0.8))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(UNetUp, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_size, out_size, 3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_size, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        channels, self.h, self.w = img_shape\n",
    "\n",
    "        self.fc = nn.Linear(latent_dim, self.h * self.w)\n",
    "\n",
    "        self.down1 = UNetDown(channels + 1, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 512)\n",
    "        self.down5 = UNetDown(512, 512)\n",
    "        self.down6 = UNetDown(512, 512)\n",
    "        self.down7 = UNetDown(512, 512, normalize=False)\n",
    "        self.up1 = UNetUp(512, 512)\n",
    "        self.up2 = UNetUp(1024, 512)\n",
    "        self.up3 = UNetUp(1024, 512)\n",
    "        self.up4 = UNetUp(1024, 256)\n",
    "        self.up5 = UNetUp(512, 128)\n",
    "        self.up6 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, stride=1, padding=1), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # Propogate noise through fc layer and reshape to img shape\n",
    "        z = self.fc(z).view(z.size(0), 1, self.h, self.w)\n",
    "        d1 = self.down1(torch.cat((x, z), 1))\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        u1 = self.up1(d7, d6)\n",
    "        u2 = self.up2(u1, d5)\n",
    "        u3 = self.up3(u2, d4)\n",
    "        u4 = self.up4(u3, d3)\n",
    "        u5 = self.up5(u4, d2)\n",
    "        u6 = self.up6(u5, d1)\n",
    "\n",
    "        return self.final(u6)    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, input_shape):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet18_model = resnet18(pretrained=False)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-3])\n",
    "\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
    "        # Output is mu and log(var) for reparameterization trick used in VAEs\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.feature_extractor(img)\n",
    "#         out = self.feature_extractor(img)\n",
    "#         out = self.pooling(out)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         mu = self.fc_mu(out)\n",
    "#         logvar = self.fc_logvar(out)\n",
    "#         return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator \n",
    "class MultiDiscriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(MultiDiscriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            return layers\n",
    "\n",
    "        channels, _, _ = input_shape\n",
    "        # Extracts discriminator models\n",
    "        self.models = nn.ModuleList()\n",
    "        for i in range(3):\n",
    "            self.models.add_module(\n",
    "                \"disc_%d\" % i,\n",
    "                nn.Sequential(\n",
    "                    *discriminator_block(channels, 64, normalize=False),\n",
    "                    *discriminator_block(64, 128),\n",
    "                    *discriminator_block(128, 256),\n",
    "                    *discriminator_block(256, 512),\n",
    "                    nn.Conv2d(512, 1, 3, padding=1)\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "        self.downsample = nn.AvgPool2d(channels, stride=2, padding=[1, 1], count_include_pad=False)\n",
    "\n",
    "#         self.downsample = nn.AvgPool2d(in_channels, stride=2, padding=[1, 1], count_include_pad=False)\n",
    "\n",
    "    def compute_loss(self, x, gt):\n",
    "        \"\"\"Computes the MSE between model output and scalar gt\"\"\"\n",
    "        loss = sum([torch.mean((out - gt) ** 2) for out in self.forward(x)])\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for m in self.models:\n",
    "            outputs.append(m(x))\n",
    "            x = self.downsample(x)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample and checkpoint directories\n",
    "os.makedirs(\"checkpoint\", exist_ok=True)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small data loaders \n",
    "batch_size = 10\n",
    "train_dl = DataLoader(train_couch_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_couch_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_s, train_x_r = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 240, 400])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 240, 400])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (3, 240, 400)  # TODO: dynamically grab this \n",
    "\n",
    "# used default parameters for this \n",
    "latent_dim = 8 \n",
    "lr = 0.01\n",
    "b1 = 0.5  # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999  # adam: decay of first order momentum of gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "mae_loss = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize generator, encoder and discriminators\n",
    "generator = Generator(latent_dim, input_shape).cuda()\n",
    "encoder = Encoder(latent_dim, input_shape).cuda()\n",
    "D_VAE = MultiDiscriminator(input_shape).cuda()\n",
    "D_LR = MultiDiscriminator(input_shape).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiDiscriminator(\n",
       "  (models): ModuleList(\n",
       "    (disc_0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): LeakyReLU(negative_slope=0.2)\n",
       "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (6): BatchNorm2d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): LeakyReLU(negative_slope=0.2)\n",
       "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (9): BatchNorm2d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): LeakyReLU(negative_slope=0.2)\n",
       "      (11): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (disc_1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): LeakyReLU(negative_slope=0.2)\n",
       "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (6): BatchNorm2d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): LeakyReLU(negative_slope=0.2)\n",
       "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (9): BatchNorm2d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): LeakyReLU(negative_slope=0.2)\n",
       "      (11): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (disc_2): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): LeakyReLU(negative_slope=0.2)\n",
       "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (6): BatchNorm2d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): LeakyReLU(negative_slope=0.2)\n",
       "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (9): BatchNorm2d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): LeakyReLU(negative_slope=0.2)\n",
       "      (11): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize weights at start of epoch training (epoch = 0)\n",
    "generator.apply(weights_init_normal)\n",
    "D_VAE.apply(weights_init_normal)\n",
    "D_LR.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_E = torch.optim.Adam(encoder.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_VAE = torch.optim.Adam(D_VAE.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_LR = torch.optim.Adam(D_LR.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation metrics \n",
    "def val_metrics(generator, valid_dl):\n",
    "    # stop training generator\n",
    "    generator.eval()\n",
    "    \n",
    "    for x_sketch, x_real in valid_dl:\n",
    "        sampled_z = Variable(Tensor(np.random.normal(0, 1, (latent_dim,\n",
    "                                                            latent_dim))))\n",
    "        x_sketch_variable = Variable(x_sketch)\n",
    "        # generate samples \n",
    "        fake_from_sketch = geneator(x_sketch_variable, sampled_z)\n",
    "        \n",
    "        # concatenate samples horizontally\n",
    "        fake_from_sketch = torch.cat([x for x in fake_from_sketch.data.cpu()], -1)\n",
    "        \n",
    "        img_sample = torch.cat((x_sketch_variable, fake_from_sketch), -1)\n",
    "        img_sample = img_sample.view(1, *img_sample.shape)\n",
    "        \n",
    "        # Concatenate with previous samples vertically\n",
    "        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n",
    "    save_image(img_samples, \"fake_imgs/%s/%s.png\" % (\"couch_from_sketch\", batches_done), nrow=8, normalize=True)\n",
    "    generator.train()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterization(mu, logvar):\n",
    "    std = torch.exp(logvar / 2)\n",
    "    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), opt.latent_dim))))\n",
    "    z = sampled_z * std + mu\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial loss\n",
    "valid = 1\n",
    "fake = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.9843, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 0.9843, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 0.9843, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[ 0.3333,  0.3333,  0.3333,  ...,  0.4980,  0.4980,  0.4980],\n",
      "         [ 0.3333,  0.3333,  0.3333,  ...,  0.4745,  0.4667,  0.4667],\n",
      "         [ 0.3333,  0.3333,  0.3333,  ...,  0.4118,  0.4039,  0.4039],\n",
      "         ...,\n",
      "         [-0.1529, -0.1529, -0.1451,  ..., -0.7098, -0.7255, -0.7490],\n",
      "         [-0.1608, -0.1608, -0.1608,  ..., -0.7098, -0.7176, -0.7333],\n",
      "         [-0.1922, -0.1922, -0.2000,  ..., -0.7098, -0.7176, -0.7412]],\n",
      "\n",
      "        [[ 0.2078,  0.2078,  0.2078,  ...,  0.2471,  0.2471,  0.2471],\n",
      "         [ 0.2078,  0.2078,  0.2078,  ...,  0.2235,  0.2157,  0.2157],\n",
      "         [ 0.2078,  0.2078,  0.2078,  ...,  0.1843,  0.1765,  0.1765],\n",
      "         ...,\n",
      "         [-0.3647, -0.3647, -0.3569,  ..., -0.8039, -0.8275, -0.8510],\n",
      "         [-0.3882, -0.3882, -0.3882,  ..., -0.8118, -0.8196, -0.8353],\n",
      "         [-0.4196, -0.4196, -0.4275,  ..., -0.8118, -0.8196, -0.8431]],\n",
      "\n",
      "        [[ 0.0824,  0.0824,  0.0824,  ...,  0.1294,  0.1294,  0.1294],\n",
      "         [ 0.0824,  0.0824,  0.0824,  ...,  0.1059,  0.0980,  0.0980],\n",
      "         [ 0.0824,  0.0824,  0.0824,  ...,  0.0588,  0.0510,  0.0510],\n",
      "         ...,\n",
      "         [-0.4980, -0.4980, -0.4902,  ..., -0.8980, -0.8980, -0.9216],\n",
      "         [-0.5137, -0.5137, -0.5137,  ..., -0.8824, -0.8902, -0.9059],\n",
      "         [-0.5451, -0.5451, -0.5529,  ..., -0.8824, -0.8902, -0.9137]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[[1.0000, 0.9922, 0.9922,  ..., 0.9922, 1.0000, 1.0000],\n",
      "         [0.9608, 1.0000, 0.9686,  ..., 1.0000, 0.9922, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 0.9922, 0.9922,  ..., 0.9922, 1.0000, 1.0000],\n",
      "         [0.9608, 1.0000, 0.9686,  ..., 1.0000, 0.9922, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 0.9922, 0.9922,  ..., 0.9922, 1.0000, 1.0000],\n",
      "         [0.9608, 1.0000, 0.9686,  ..., 1.0000, 0.9922, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[ 0.7333,  0.7333,  0.7412,  ...,  0.2706,  0.2863,  0.3098],\n",
      "         [ 0.7333,  0.7333,  0.7412,  ...,  0.2706,  0.2941,  0.3098],\n",
      "         [ 0.7333,  0.7333,  0.7412,  ...,  0.2784,  0.3020,  0.3098],\n",
      "         ...,\n",
      "         [ 0.0353,  0.0902,  0.0431,  ...,  0.3804,  0.3961,  0.3961],\n",
      "         [ 0.0353,  0.0745,  0.0275,  ...,  0.3412,  0.3569,  0.3647],\n",
      "         [ 0.0196,  0.0588,  0.0196,  ...,  0.3098,  0.3255,  0.3412]],\n",
      "\n",
      "        [[ 0.8039,  0.8039,  0.8118,  ...,  0.3020,  0.3176,  0.3490],\n",
      "         [ 0.8039,  0.8039,  0.8118,  ...,  0.3020,  0.3255,  0.3490],\n",
      "         [ 0.8039,  0.8039,  0.8118,  ...,  0.3098,  0.3333,  0.3412],\n",
      "         ...,\n",
      "         [-0.6314, -0.5765, -0.6078,  ..., -0.4510, -0.4353, -0.4353],\n",
      "         [-0.6314, -0.5922, -0.6235,  ..., -0.4902, -0.4745, -0.4510],\n",
      "         [-0.6471, -0.6078, -0.6314,  ..., -0.5216, -0.4902, -0.4745]],\n",
      "\n",
      "        [[ 0.8431,  0.8431,  0.8510,  ...,  0.1686,  0.1843,  0.1922],\n",
      "         [ 0.8431,  0.8431,  0.8510,  ...,  0.1843,  0.1922,  0.1922],\n",
      "         [ 0.8431,  0.8431,  0.8510,  ...,  0.1922,  0.2157,  0.2078],\n",
      "         ...,\n",
      "         [-0.3882, -0.3333, -0.3725,  ..., -0.1529, -0.1373, -0.1373],\n",
      "         [-0.3882, -0.3490, -0.3882,  ..., -0.1922, -0.1765, -0.1608],\n",
      "         [-0.4039, -0.3647, -0.3961,  ..., -0.2235, -0.2000, -0.1843]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[ 1.0000,  1.0000,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 0.9922,  0.9922,  0.9765,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 0.9922, -0.9765, -0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  0.9922],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  0.9843,  1.0000]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 0.9922,  0.9922,  0.9765,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 0.9922, -0.9765, -0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  0.9922],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  0.9843,  1.0000]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 0.9922,  0.9922,  0.9765,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 0.9922, -0.9765, -0.9843,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  0.9922],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  0.9843,  1.0000]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[[-0.4039, -0.4980, -0.4824,  ..., -0.5451, -0.6784, -0.5373],\n",
      "         [-0.3882, -0.4745, -0.5137,  ..., -0.5922, -0.6549, -0.5216],\n",
      "         [-0.4980, -0.5529, -0.5922,  ..., -0.6392, -0.5373, -0.4510],\n",
      "         ...,\n",
      "         [-0.3882, -0.3176, -0.3333,  ..., -0.2549, -0.2392, -0.2784],\n",
      "         [-0.3490, -0.3020, -0.3176,  ..., -0.2627, -0.4275, -0.2941],\n",
      "         [-0.3333, -0.3176, -0.3412,  ..., -0.3255, -0.4902, -0.5216]],\n",
      "\n",
      "        [[-0.9137, -0.9843, -0.9451,  ..., -0.5137, -0.6471, -0.5059],\n",
      "         [-0.8667, -0.9373, -0.9451,  ..., -0.5608, -0.6235, -0.4902],\n",
      "         [-0.9059, -0.9451, -0.9529,  ..., -0.6078, -0.5059, -0.4196],\n",
      "         ...,\n",
      "         [-0.3333, -0.2627, -0.2784,  ..., -0.2941, -0.2627, -0.3020],\n",
      "         [-0.2941, -0.2471, -0.2627,  ..., -0.3020, -0.4431, -0.3020],\n",
      "         [-0.2784, -0.2627, -0.2863,  ..., -0.3490, -0.5059, -0.5294]],\n",
      "\n",
      "        [[-0.8824, -0.9608, -0.9137,  ..., -0.6235, -0.7569, -0.6235],\n",
      "         [-0.8275, -0.9059, -0.9216,  ..., -0.6706, -0.7333, -0.6078],\n",
      "         [-0.8745, -0.9216, -0.9373,  ..., -0.7176, -0.6157, -0.5373],\n",
      "         ...,\n",
      "         [-0.3882, -0.3176, -0.3333,  ..., -0.2392, -0.2078, -0.2471],\n",
      "         [-0.3490, -0.3020, -0.3176,  ..., -0.2471, -0.4039, -0.2627],\n",
      "         [-0.3333, -0.3176, -0.3412,  ..., -0.2941, -0.4667, -0.4902]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[ 0.5137,  0.4980,  0.5294,  ..., -0.3333, -0.3333, -0.3490],\n",
      "         [ 0.5216,  0.5059,  0.5373,  ..., -0.3176, -0.3255, -0.3412],\n",
      "         [ 0.5294,  0.5216,  0.5451,  ..., -0.3176, -0.3255, -0.3412],\n",
      "         ...,\n",
      "         [ 0.7020,  0.7098,  0.7176,  ...,  0.3490,  0.3255,  0.3098],\n",
      "         [ 0.7098,  0.7255,  0.7255,  ...,  0.3569,  0.3412,  0.3412],\n",
      "         [ 0.6706,  0.6941,  0.6941,  ...,  0.1294,  0.0510,  0.0275]],\n",
      "\n",
      "        [[-0.2078, -0.2235, -0.1922,  ..., -0.7255, -0.7255, -0.7412],\n",
      "         [-0.2000, -0.2157, -0.1843,  ..., -0.7098, -0.7176, -0.7333],\n",
      "         [-0.1922, -0.2000, -0.1765,  ..., -0.7098, -0.7176, -0.7333],\n",
      "         ...,\n",
      "         [ 0.1922,  0.2000,  0.2078,  ..., -0.0353, -0.0667, -0.0824],\n",
      "         [ 0.1843,  0.2000,  0.2157,  ..., -0.0275, -0.0353, -0.0353],\n",
      "         [ 0.1451,  0.1686,  0.1843,  ..., -0.2549, -0.3255, -0.3490]],\n",
      "\n",
      "        [[-0.8118, -0.8275, -0.7961,  ..., -0.9843, -0.9843, -1.0000],\n",
      "         [-0.8039, -0.8196, -0.7882,  ..., -0.9686, -0.9765, -0.9922],\n",
      "         [-0.7961, -0.8039, -0.7804,  ..., -0.9686, -0.9765, -0.9922],\n",
      "         ...,\n",
      "         [-0.2627, -0.2549, -0.2471,  ..., -0.3490, -0.3569, -0.3725],\n",
      "         [-0.2471, -0.2314, -0.2235,  ..., -0.3412, -0.3333, -0.3333],\n",
      "         [-0.2863, -0.2627, -0.2549,  ..., -0.5686, -0.6235, -0.6471]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[ 0.0275,  0.1373, -0.1216,  ...,  0.3882,  0.2235,  0.3569],\n",
      "         [-0.1216, -0.0902,  0.0588,  ...,  0.1686,  0.1686,  0.5216],\n",
      "         [-0.0353,  0.0196,  0.2392,  ...,  0.1608,  0.1922,  0.3490],\n",
      "         ...,\n",
      "         [ 0.9137,  0.9137,  0.9137,  ...,  0.8196,  0.8118,  0.7882],\n",
      "         [ 0.9137,  0.9137,  0.9137,  ...,  0.8275,  0.8118,  0.7961],\n",
      "         [ 0.9373,  0.9294,  0.9216,  ...,  0.8275,  0.8118,  0.8118]],\n",
      "\n",
      "        [[ 0.3490,  0.4431,  0.1137,  ...,  0.5765,  0.3882,  0.5137],\n",
      "         [ 0.1922,  0.2000,  0.2941,  ...,  0.3569,  0.3333,  0.6784],\n",
      "         [ 0.2549,  0.3020,  0.4745,  ...,  0.3647,  0.3647,  0.5216],\n",
      "         ...,\n",
      "         [ 0.9137,  0.9137,  0.9137,  ...,  0.8353,  0.8275,  0.8039],\n",
      "         [ 0.9137,  0.9137,  0.9137,  ...,  0.8431,  0.8275,  0.8118],\n",
      "         [ 0.9373,  0.9294,  0.9216,  ...,  0.8431,  0.8275,  0.8275]],\n",
      "\n",
      "        [[-0.6549, -0.4588, -0.5608,  ..., -0.0353, -0.1373,  0.0196],\n",
      "         [-0.7725, -0.6863, -0.3804,  ..., -0.2392, -0.1922,  0.1843],\n",
      "         [-0.6549, -0.5451, -0.2157,  ..., -0.2078, -0.1451,  0.0196],\n",
      "         ...,\n",
      "         [ 0.9137,  0.9137,  0.9137,  ...,  0.8275,  0.8196,  0.7961],\n",
      "         [ 0.9137,  0.9137,  0.9137,  ...,  0.8353,  0.8039,  0.7882],\n",
      "         [ 0.9373,  0.9294,  0.9216,  ...,  0.8353,  0.8039,  0.8039]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  0.9765,  ..., -1.0000, -0.9922,  1.0000],\n",
      "         [ 0.9922,  1.0000,  0.9922,  ...,  0.9922, -1.0000,  0.9765],\n",
      "         [ 1.0000,  0.9843,  1.0000,  ...,  1.0000,  0.9843,  1.0000]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  0.9765,  ..., -1.0000, -0.9922,  1.0000],\n",
      "         [ 0.9922,  1.0000,  0.9922,  ...,  0.9922, -1.0000,  0.9765],\n",
      "         [ 1.0000,  0.9843,  1.0000,  ...,  1.0000,  0.9843,  1.0000]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  0.9765,  ..., -1.0000, -0.9922,  1.0000],\n",
      "         [ 0.9922,  1.0000,  0.9922,  ...,  0.9922, -1.0000,  0.9765],\n",
      "         [ 1.0000,  0.9843,  1.0000,  ...,  1.0000,  0.9843,  1.0000]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[[ 0.2392,  0.1922,  0.0745,  ...,  0.9137,  0.9216,  0.9216],\n",
      "         [ 0.1765,  0.1529,  0.1059,  ...,  0.9137,  0.9216,  0.9216],\n",
      "         [ 0.1294,  0.1059,  0.0824,  ...,  0.9137,  0.9216,  0.9216],\n",
      "         ...,\n",
      "         [ 0.5294,  0.5373,  0.5451,  ...,  0.0353, -0.1059, -0.1686],\n",
      "         [ 0.5137,  0.5216,  0.5373,  ..., -0.0745, -0.2392, -0.3020],\n",
      "         [ 0.4980,  0.5137,  0.5373,  ..., -0.1137, -0.2941, -0.3647]],\n",
      "\n",
      "        [[ 0.1373,  0.1059,  0.0196,  ...,  0.9059,  0.9137,  0.9137],\n",
      "         [ 0.0902,  0.1059,  0.0667,  ...,  0.9059,  0.9137,  0.9137],\n",
      "         [ 0.0980,  0.1059,  0.1059,  ...,  0.9059,  0.9137,  0.9137],\n",
      "         ...,\n",
      "         [ 0.3725,  0.3804,  0.3882,  ..., -0.1686, -0.3098, -0.3725],\n",
      "         [ 0.3412,  0.3490,  0.3804,  ..., -0.2784, -0.4431, -0.5059],\n",
      "         [ 0.3255,  0.3412,  0.3804,  ..., -0.3176, -0.4980, -0.5686]],\n",
      "\n",
      "        [[ 0.0039, -0.0353, -0.1294,  ...,  0.6706,  0.6784,  0.6784],\n",
      "         [-0.0667, -0.0667, -0.1059,  ...,  0.6706,  0.6784,  0.6784],\n",
      "         [-0.0980, -0.0980, -0.1059,  ...,  0.6706,  0.6784,  0.6784],\n",
      "         ...,\n",
      "         [ 0.1843,  0.1922,  0.2000,  ..., -0.3804, -0.5216, -0.5843],\n",
      "         [ 0.1608,  0.1686,  0.1922,  ..., -0.4745, -0.6392, -0.7020],\n",
      "         [ 0.1451,  0.1608,  0.1922,  ..., -0.5137, -0.6941, -0.7647]]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# debugging training \n",
    "for x_sketch, x_real in zip(train_x_s, train_x_r):\n",
    "    x_sketch_variable = Variable(x_sketch)\n",
    "    x_real_variable = Variable(x_real)\n",
    "    print(x_sketch_variable)\n",
    "    print(x_real_variable)\n",
    "    # train generator and encoder \n",
    "#     optimizer_E.zero_grad()\n",
    "#     optimizer_G.zero_grad()\n",
    "    \n",
    "#     # cVAE-GAN\n",
    "    \n",
    "#     # Produce output using encoding of real image \n",
    "#     mu, logvar = encoder(x_real_variable)\n",
    "#     encoded_z = reparameterization(mu, logvar)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
