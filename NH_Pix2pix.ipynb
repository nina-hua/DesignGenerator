{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "\n",
    "This is a first attempt at creating a simple GAN model following the documentation of *Deep Learning with Pytorch* by Vishnu Subramanian and this Medium article: https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f\n",
    "\n",
    "Certain model parameters and structure will be based off of the pix2pix model implemented in *Image-to-Image Translation with Conditional Adversarial Networks* by Isola, et al.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time \n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load re-sized images and their corresponding sketches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/couch_gan\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign paths \n",
    "real_img_path = Path('./data/og_resized')\n",
    "sketch_img_path = Path('./data/sketch_resized')\n",
    "fake_img_path = Path('./data/fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files into list  \n",
    "real_img_files = [f for f in real_img_path.iterdir()]\n",
    "sketch_img_files = [f for f in sketch_img_path.iterdir()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get image names of training and test files (assigned by Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_names = [f'00000{i}.jpg' for i in range(203, 303)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign paths to appropriate lists \n",
    "train_real_paths = []\n",
    "train_sketch_paths = []\n",
    "test_real_paths = []\n",
    "test_sketch_paths = []\n",
    "\n",
    "for f in real_img_files:\n",
    "    if f.parts[-1] in test_file_names:\n",
    "        test_real_paths.append(f)\n",
    "    else:\n",
    "        train_real_paths.append(f)\n",
    "\n",
    "for f in sketch_img_files:\n",
    "    if f.parts[-1] in test_file_names:\n",
    "        test_sketch_paths.append(f)\n",
    "    else:\n",
    "        train_sketch_paths.append(f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any inapporpiate files like .DS_Store \n",
    "train_sketch_paths = [file for file in train_sketch_paths if not file.parts[-1].startswith('.')]\n",
    "test_real_paths = [file for file in test_real_paths if not file.parts[-1].startswith('.')]\n",
    "test_sketch_paths = [file for file in test_sketch_paths if not file.parts[-1].startswith('.')]\n",
    "train_real_paths = [file for file in train_real_paths if not file.parts[-1].startswith('.')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that the order of real and sketch paths are the same \n",
    "counter = 0\n",
    "for r, f in zip(train_real_paths, train_sketch_paths):\n",
    "    if r.parts[-1] == r.parts[-1]:\n",
    "        counter += 1\n",
    "assert(counter == 900)\n",
    "\n",
    "counter = 0\n",
    "for r, f in zip(test_real_paths, test_sketch_paths):\n",
    "    if r.parts[-1] == r.parts[-1]:\n",
    "        counter += 1\n",
    "assert(counter == 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Couch Dataset  and Dataloaders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: No data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getX(path):\n",
    "    x = cv2.imread(str(path)).astype(np.float32)  # reading image \n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)/255  # convert from BGR to RGB\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(im):\n",
    "    \"\"\"Normalizes images \"\"\"\n",
    "    imagenet_stats = np.array([[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]])\n",
    "    return (im - imagenet_stats[0])/imagenet_stats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDataset(Dataset):\n",
    "    def __init__(self, real_files, sketch_files):\n",
    "        self.real_files = real_files\n",
    "        self.sketch_files = sketch_files\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.real_files)  # should be same number as sketch files \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        real_path = self.real_files[idx]\n",
    "        sketch_path = self.sketch_files[idx]\n",
    "        img_name = sketch_path.parts[-1]\n",
    "        \n",
    "        x_real = getX(real_path)\n",
    "        x_sketch = getX(sketch_path)\n",
    "        \n",
    "        # normalize \n",
    "        x_real = normalize(x_real)\n",
    "        x_sketch = normalize(x_sketch)\n",
    "        \n",
    "        # roll axis (channels, height, width)\n",
    "        x_real = np.rollaxis(x_real, 2)\n",
    "        x_sketch = np.rollaxis(x_sketch, 2)\n",
    "        \n",
    "        return x_sketch, x_real\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make datasets \n",
    "train_couch_ds = CouchDataset(real_files=train_real_paths, sketch_files=train_sketch_paths)\n",
    "test_couch_ds = CouchDataset(real_files=test_real_paths, sketch_files=test_sketch_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect content \n",
    "x_s, x_r = train_couch_ds[20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 1.        ,  1.        ,  1.        , ...,  0.99215686,\n",
       "           1.        ,  0.99215686],\n",
       "         [ 1.        ,  1.        ,  1.        , ..., -1.        ,\n",
       "          -0.98431372,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  0.97647059,\n",
       "           0.97647059,  1.        ],\n",
       "         ...,\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "           1.        ,  1.        ]],\n",
       " \n",
       "        [[ 1.        ,  1.        ,  1.        , ...,  0.99215686,\n",
       "           1.        ,  0.99215686],\n",
       "         [ 1.        ,  1.        ,  1.        , ..., -1.        ,\n",
       "          -0.98431372,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  0.97647059,\n",
       "           0.97647059,  1.        ],\n",
       "         ...,\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "           1.        ,  1.        ]],\n",
       " \n",
       "        [[ 1.        ,  1.        ,  1.        , ...,  0.99215686,\n",
       "           1.        ,  0.99215686],\n",
       "         [ 1.        ,  1.        ,  1.        , ..., -1.        ,\n",
       "          -0.98431372,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  0.97647059,\n",
       "           0.97647059,  1.        ],\n",
       "         ...,\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "           1.        ,  1.        ]]]),\n",
       " array([[[ 0.87450981,  0.89019608,  0.9137255 , ...,  0.32549024,\n",
       "           0.3176471 ,  0.3176471 ],\n",
       "         [ 0.88235295,  0.89803922,  0.9137255 , ..., -0.09019607,\n",
       "          -0.03529412,  0.0196079 ],\n",
       "         [ 0.84313726,  0.85882354,  0.86666667, ..., -0.36470586,\n",
       "          -0.34117645, -0.31764704],\n",
       "         ...,\n",
       "         [-0.45098037, -0.4588235 , -0.4588235 , ..., -0.02745098,\n",
       "          -0.05098039, -0.06666666],\n",
       "         [-0.46666664, -0.48235291, -0.47450978, ..., -0.05098039,\n",
       "          -0.05882353, -0.05882353],\n",
       "         [-0.4588235 , -0.47450978, -0.47450978, ..., -0.00392157,\n",
       "          -0.01176471, -0.00392157]],\n",
       " \n",
       "        [[ 0.71764708,  0.73333335,  0.75686276, ...,  0.18431377,\n",
       "           0.17647064,  0.17647064],\n",
       "         [ 0.74117649,  0.75686276,  0.77254903, ..., -0.23137254,\n",
       "          -0.17647058, -0.12156862],\n",
       "         [ 0.70980394,  0.72549021,  0.73333335, ..., -0.52156863,\n",
       "          -0.49803919, -0.47450978],\n",
       "         ...,\n",
       "         [-0.4588235 , -0.46666664, -0.46666664, ..., -0.02745098,\n",
       "          -0.05098039, -0.06666666],\n",
       "         [-0.47450978, -0.49019605, -0.48235291, ..., -0.05098039,\n",
       "          -0.05882353, -0.05882353],\n",
       "         [-0.46666664, -0.48235291, -0.48235291, ..., -0.00392157,\n",
       "          -0.01176471, -0.00392157]],\n",
       " \n",
       "        [[ 0.64705884,  0.66274512,  0.68627453, ...,  0.07450986,\n",
       "           0.06666672,  0.06666672],\n",
       "         [ 0.66274512,  0.67843139,  0.69411767, ..., -0.34117645,\n",
       "          -0.28627449, -0.23137254],\n",
       "         [ 0.64705884,  0.66274512,  0.67058825, ..., -0.60784313,\n",
       "          -0.58431372, -0.56078431],\n",
       "         ...,\n",
       "         [-0.49019605, -0.49803919, -0.49803919, ..., -0.02745098,\n",
       "          -0.05098039, -0.06666666],\n",
       "         [-0.50588235, -0.52156863, -0.51372549, ..., -0.05098039,\n",
       "          -0.05882353, -0.05882353],\n",
       "         [-0.49803919, -0.51372549, -0.51372549, ..., -0.00392157,\n",
       "          -0.01176471, -0.00392157]]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_s, x_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 240, 400), (3, 240, 400))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 channels for RGB\n",
    "# 240 rows (height)\n",
    "# 400 columns (width)\n",
    "x_s.shape, x_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on Pix2pix GAN \n",
    "## https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/pix2pix/models.py\n",
    "\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 3, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        layers = [\n",
    "#             nn.ConvTranspose2d(in_size, out_size, 3, 2, 1, bias=False),\n",
    "#             nn.ConvTranspose2d(in_size, out_size, 3, 2, 1, output_padding=1, bias=False), \n",
    "#             nn.ConvTranspose2d(in_size, out_size, 3, padding=1, output_padding=(1,2),bias=False),\n",
    "            nn.ConvTranspose2d(in_size, out_size, 3, 2, 1, output_padding=1,bias=True),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        print(\"model x shape\", x.shape)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
    "\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up5 = UNetUp(1024, 256)\n",
    "        self.up6 = UNetUp(512, 128)\n",
    "        self.up7 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, 3, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # U-Net generator with skip connections from encoder to decoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        print(\"x\", x.shape)\n",
    "        print(\"d1\",d1.shape)\n",
    "        print(\"d2\",d2.shape)\n",
    "        print(\"d3\",d3.shape)\n",
    "        print(\"d4\",d4.shape)\n",
    "        print(\"d5\",d5.shape)\n",
    "        print(\"d6\",d6.shape)\n",
    "        print(\"d7\",d7.shape)\n",
    "        print(\"d8\",d8.shape)\n",
    "        \n",
    "        u1 = self.up1(d8, d7)\n",
    "        print(\"u1\", u1.shape)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        print(\"u2\", u2.shape)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        print(\"u3\", u3.shape)\n",
    "\n",
    "        u4 = self.up4(u3, d4)\n",
    "        print(\"u4\", u4.shape)\n",
    "\n",
    "        u5 = self.up5(u4, d3)\n",
    "        print(\"u5\", u5.shape)\n",
    "\n",
    "        u6 = self.up6(u5, d2)\n",
    "        print(\"u6\", u6.shape)\n",
    "\n",
    "        u7 = self.up7(u6, d1)\n",
    "        print(\"u7\", u7.shape)\n",
    "\n",
    "        return self.final(u7)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels * 2, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 3, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UNetDown(nn.Module):\n",
    "#     def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "#         super(UNetDown, self).__init__()\n",
    "#         layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "#         if normalize:\n",
    "#             layers.append(nn.InstanceNorm2d(out_size))\n",
    "#         layers.append(nn.LeakyReLU(0.2))\n",
    "#         if dropout:\n",
    "#             layers.append(nn.Dropout(dropout))\n",
    "#         self.model = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "\n",
    "# class UNetUp(nn.Module):\n",
    "#     def __init__(self, in_size, out_size, dropout=0.0):\n",
    "#         super(UNetUp, self).__init__()\n",
    "#         layers = [\n",
    "#             nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "#             nn.InstanceNorm2d(out_size),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#         ]\n",
    "#         if dropout:\n",
    "#             layers.append(nn.Dropout(dropout))\n",
    "\n",
    "#         self.model = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x, skip_input):\n",
    "#         x = self.model(x)\n",
    "#         x = torch.cat((x, skip_input), 1)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class GeneratorUNet(nn.Module):\n",
    "#     def __init__(self, in_channels=3, out_channels=3):\n",
    "#         super(GeneratorUNet, self).__init__()\n",
    "\n",
    "#         self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
    "#         self.down2 = UNetDown(64, 128)\n",
    "#         self.down3 = UNetDown(128, 256)\n",
    "#         self.down4 = UNetDown(256, 512, dropout=0.5)\n",
    "#         self.down5 = UNetDown(512, 512, dropout=0.5)\n",
    "#         self.down6 = UNetDown(512, 512, dropout=0.5)\n",
    "#         self.down7 = UNetDown(512, 512, dropout=0.5)\n",
    "#         self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
    "\n",
    "#         self.up1 = UNetUp(512, 512, dropout=0.5)\n",
    "#         self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
    "#         self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
    "#         self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
    "#         self.up5 = UNetUp(1024, 256)\n",
    "#         self.up6 = UNetUp(512, 128)\n",
    "#         self.up7 = UNetUp(256, 64)\n",
    "\n",
    "#         self.final = nn.Sequential(\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "#             nn.Conv2d(128, out_channels, 4, padding=1),\n",
    "#             nn.Tanh(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # U-Net generator with skip connections from encoder to decoder\n",
    "#         d1 = self.down1(x)\n",
    "#         d2 = self.down2(d1)\n",
    "#         d3 = self.down3(d2)\n",
    "#         d4 = self.down4(d3)\n",
    "#         d5 = self.down5(d4)\n",
    "#         d6 = self.down6(d5)\n",
    "#         d7 = self.down7(d6)\n",
    "#         d8 = self.down8(d7)\n",
    "#         u1 = self.up1(d8, d7)\n",
    "#         u2 = self.up2(u1, d6)\n",
    "#         u3 = self.up3(u2, d5)\n",
    "#         u4 = self.up4(u3, d4)\n",
    "#         u5 = self.up5(u4, d3)\n",
    "#         u6 = self.up6(u5, d2)\n",
    "#         u7 = self.up7(u6, d1)\n",
    "\n",
    "#         return self.final(u7)\n",
    "\n",
    "\n",
    "# ##############################\n",
    "# #        Discriminator\n",
    "# ##############################\n",
    "\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, in_channels=3):\n",
    "#         super(Discriminator, self).__init__()\n",
    "\n",
    "#         def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "#             \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "#             layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "#             if normalization:\n",
    "#                 layers.append(nn.InstanceNorm2d(out_filters))\n",
    "#             layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "#             return layers\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             *discriminator_block(in_channels * 2, 64, normalization=False),\n",
    "#             *discriminator_block(64, 128),\n",
    "#             *discriminator_block(128, 256),\n",
    "#             *discriminator_block(256, 512),\n",
    "#             nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "#             nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, img_A, img_B):\n",
    "#         # Concatenate image and condition image by channels to produce input\n",
    "#         img_input = torch.cat((img_A, img_B), 1)\n",
    "#         return self.model(img_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample and checkpoint directories\n",
    "os.makedirs(\"checkpoint\", exist_ok=True)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "os.makedirs(\"fake_imgs\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small data loaders \n",
    "# batch_size = 10\n",
    "batch_size = 1\n",
    "train_dl = DataLoader(train_couch_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_couch_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_s, train_x_r = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 240, 400])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 240, 400])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (3, 240, 400)  # TODO: dynamically grab this \n",
    "\n",
    "# used default parameters for this \n",
    "latent_dim = 8 \n",
    "lr = 0.01\n",
    "b1 = 0.5  # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999  # adam: decay of first order momentum of gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate output of image discriminator (PatchGAN)\n",
    "img_height = 240\n",
    "img_width = 400\n",
    "patch = (1, img_height // 2 ** 4, img_width // 2 ** 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator and loss functions \n",
    "generator = GeneratorUNet().cuda()\n",
    "discriminator = Discriminator().cuda()\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_pixelwise = torch.nn.L1Loss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(6, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (11): ZeroPad2d(padding=(1, 0, 1, 0), value=0.0)\n",
       "    (12): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor type\n",
    "Tensor = torch.cuda.FloatTensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    x_sketch, x_real = next(iter(test_dl))\n",
    "    x_sketch_var = Variable(x_sketch.type(Tensor))\n",
    "    x_real_var = Variable(x_real.type(Tensor))\n",
    "    fake_from_sketch = generator(x_sketch_var)\n",
    "    \n",
    "    img_sample = torch.cat((x_sketch_var.data, fake_from_sketch.data, x_real_var.data), -2)\n",
    "    save_image(img_sample, \"fake_imgs/%s/%s.png\" % (\"pix2pix\", batches_done), nrow=5, normalize=True)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_sketch, train_x_real = next(iter(train_dl))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for x_sketch, x_real in zip(train_x_sketch, train_x_real):\n",
    "#     # model inputs \n",
    "#     x_sketch_var = Variable(x_sketch.type(Tensor))\n",
    "#     x_real_var = Variable(x_real.type(Tensor))\n",
    "#     print(\"sketch var\", x_sketch.shape)\n",
    "#     print(\"real var\", x_real.shape)\n",
    "    \n",
    "# #     # adveserial ground truths \n",
    "# #     # iffy about size; should it be the number of channels\n",
    "#     valid = Variable(Tensor(np.ones((x_sketch_var.size(0), *patch))), requires_grad=False)\n",
    "#     fake = Variable(Tensor(np.zeros((x_sketch_var.size(0), *patch))), requires_grad=False)\n",
    "    \n",
    "# #     # train generators \n",
    "#     optimizer_G.zero_grad()\n",
    "    \n",
    "# #     # GAN loss\n",
    "#     fake_sketch = generator(x_real_var)\n",
    "# #     pred_fake = discriminator(fake_sketch, x_real_var)\n",
    "# #     loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "    \n",
    "# #     # Pixel-wise loss\n",
    "# #     loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "\n",
    "# #     # Total loss\n",
    "# #     loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "# #     loss_G.backward()\n",
    "\n",
    "# #     optimizer_G.step()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(train_dl):\n",
    "#     print(\"iteration\", i)\n",
    "#     print(\"sketch\", batch[0])\n",
    "#     print(\"real\",batch[1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1, 3, 240, 400])\n",
      "d1 torch.Size([1, 64, 120, 200])\n",
      "d2 torch.Size([1, 128, 60, 100])\n",
      "d3 torch.Size([1, 256, 30, 50])\n",
      "d4 torch.Size([1, 512, 15, 25])\n",
      "d5 torch.Size([1, 512, 8, 13])\n",
      "d6 torch.Size([1, 512, 4, 7])\n",
      "d7 torch.Size([1, 512, 2, 4])\n",
      "d8 torch.Size([1, 512, 1, 2])\n",
      "model x shape torch.Size([1, 512, 2, 4])\n",
      "u1 torch.Size([1, 1024, 2, 4])\n",
      "model x shape torch.Size([1, 512, 4, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 7 and 8 in dimension 3 at /opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/generic/THCTensorMath.cu:71",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-359-7e57d0642d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# GAN loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfake_sketch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_real_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-340-8521dad4cf25>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"u1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mu2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"u2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mu3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-340-8521dad4cf25>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip_input)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model x shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 7 and 8 in dimension 3 at /opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/generic/THCTensorMath.cu:71"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dl):\n",
    "    # model inputs \n",
    "    x_sketch_var = Variable(batch[0].type(Tensor))\n",
    "    x_real_var = Variable(batch[1].type(Tensor))\n",
    "    \n",
    "    # adveserial ground truths \n",
    "    # iffy about size; should it be the number of channels\n",
    "    valid = Variable(Tensor(np.ones((x_sketch_var.size(0), *patch))), requires_grad=False)\n",
    "    fake = Variable(Tensor(np.zeros((x_sketch_var.size(0), *patch))), requires_grad=False)\n",
    "    \n",
    "    # train generators \n",
    "    optimizer_G.zero_grad()\n",
    "    \n",
    "    # GAN loss\n",
    "    fake_sketch = generator(x_real_var)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment for next time:\n",
    "Need to generate a new U-net architecture so that it dynamically matches the transposed convolution's output (height and width) and the previous 'down' layer. Adding the output_padding values manually is not feasible since there doesn't seem to be a linear relationship  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
